{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5574de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e68b8b",
   "metadata": {},
   "source": [
    "En este script, utilizo las bibliotecas `numpy` y `cv2` para manejar operaciones de procesamiento de imágenes con OpenCV en Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rostro = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "output_folder = 'C:\\\\Users\\\\Janin\\\\Documents\\\\IA\\\\emociones\\\\felicidad2\\\\'\n",
    "files = os.listdir(output_folder)\n",
    "# Extrae los números de los nombres de archivos existentes y encuentra el máximo\n",
    "if files:\n",
    "    i = max([int(f.split('feliz')[1].split('.png')[0]) for f in files if 'feliz' in f]) + 1\n",
    "else:\n",
    "    i = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    rostros = rostro.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x, y, w, h) in rostros:\n",
    "        frame2 = frame[y:y+h, x:x+w]\n",
    "        frame2 = cv.resize(frame2, (100, 100), interpolation=cv.INTER_AREA)\n",
    "        cv.imwrite(output_folder + 'feliz' + str(i) + '.png', frame2)\n",
    "        i += 1\n",
    "        \n",
    "    cv.imshow('rostros', frame)\n",
    "    k = cv.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b9c51",
   "metadata": {},
   "source": [
    "### Script de detección de rostros con OpenCV\n",
    "\n",
    "Desarrollé este script para capturar video en tiempo real usando OpenCV, aprovechando la cámara integrada. Implementé un clasificador Haar (haarcascade_frontalface_alt.xml) para identificar rostros en los fotogramas en tiempo real. Cada rostro detectado es recortado y redimensionado a 100x100 píxeles, y luego se guarda automáticamente en una carpeta específica con nombres secuenciales que comienzan con 'feliz'. El proceso se muestra en vivo y finaliza al presionar la tecla 'Esc'. Además, el script continua la numeración de archivos de forma secuencial basándose en archivos previamente guardados en la carpeta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def obtenerModelo(method, facesData, labels):\n",
    "    if method == 'FisherFaces': \n",
    "        emotion_recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "\n",
    "        # Entrenando el reconocedor de rostros\n",
    "        print(\"Entrenando ( \"+method+\" )...\")\n",
    "        inicio = time.time()\n",
    "        emotion_recognizer.train(facesData, np.array(labels))\n",
    "        tiempoEntrenamiento = time.time()-inicio\n",
    "        print(\"Tiempo de entrenamiento ( \"+method+\" ): \", tiempoEntrenamiento)\n",
    "\n",
    "        # Almacenando el modelo obtenido\n",
    "        emotion_recognizer.write(\"modelo\"+method+\".xml\")\n",
    "\n",
    "dataPath = 'C:\\\\Users\\\\Janin\\\\Documents\\\\IA\\\\emociones' # Cambia a la ruta donde hayas almacenado Data\n",
    "emotionsList = os.listdir(dataPath)\n",
    "print('Lista de emociones: ', emotionsList)\n",
    "\n",
    "labels = []\n",
    "facesData = []\n",
    "label = 0\n",
    "\n",
    "for nameDir in emotionsList:\n",
    "    emotionsPath = os.path.join(dataPath, nameDir)\n",
    "\n",
    "    for fileName in os.listdir(emotionsPath):\n",
    "        labels.append(label)\n",
    "        facesData.append(cv2.imread(os.path.join(emotionsPath, fileName), 0))\n",
    "\n",
    "    label += 1\n",
    "\n",
    "# Llamando a la función para entrenar y almacenar solo el modelo FisherFaces\n",
    "obtenerModelo('FisherFaces', facesData, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcaf77",
   "metadata": {},
   "source": [
    "# Entrenamiento de reconocedor de emociones con FisherFaces\n",
    "\n",
    "En este script, utilizo OpenCV para entrenar un reconocedor de emociones utilizando el método FisherFaces. Cargo todas las imágenes desde un directorio especificado que contiene imágenes etiquetadas por emociones. Después de leer las imágenes y asignar etiquetas, entreno el modelo utilizando los datos de las caras y las etiquetas correspondientes. El tiempo que toma entrenar el modelo se mide y se imprime. Finalmente, guardo el modelo entrenado en un archivo XML para su uso futuro. Este proceso ayuda a desarrollar un sistema capaz de reconocer emociones en imágenes faciales de manera eficiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d758b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Método usado para el entrenamiento y lectura del modelo\n",
    "method = 'FisherFaces'\n",
    "\n",
    "# Crea el reconocedor de rostros según el método seleccionado\n",
    "if method == 'FisherFaces':\n",
    "    emotion_recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "\n",
    "# Lee el modelo entrenado\n",
    "emotion_recognizer.read('modelo' + method + '.xml')\n",
    "\n",
    "dataPath = 'C:\\\\Users\\\\Janin\\\\Documents\\\\IA\\\\emociones'  # Cambia a la ruta donde hayas almacenado Data\n",
    "imagePaths = os.listdir(dataPath)\n",
    "print('imagePaths=', imagePaths)\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "faceClassif = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    auxFrame = gray.copy()\n",
    "\n",
    "    faces = faceClassif.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        rostro = auxFrame[y:y + h, x:x + w]\n",
    "        rostro = cv2.resize(rostro, (100, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        result = emotion_recognizer.predict(rostro)\n",
    "\n",
    "        cv2.putText(frame, '{}'.format(result), (x, y - 5), 1, 1.3, (255, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        if method == 'FisherFaces':\n",
    "            if result[1] < 70:\n",
    "                cv2.putText(frame, '{}'.format(imagePaths[result[0]]), (x, y - 25), 2, 1.1, (0, 255, 0), 1,\n",
    "                            cv2.LINE_AA)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, 'No identificado', (x, y - 20), 2, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635c08d",
   "metadata": {},
   "source": [
    "# Detección en tiempo real \n",
    "\n",
    "En este script, implemento un sistema de reconocimiento facial en tiempo real utilizando la biblioteca OpenCV. Primero, cargo un modelo preentrenado de reconocimiento de emociones usando FisherFaces. Luego, inicio la captura de video en tiempo real usando la webcam. Para cada cuadro capturado, detecto rostros y aplico el modelo para predecir la emoción. Los resultados se muestran en la pantalla con un cuadro alrededor del rostro y una etiqueta que indica la emoción detectada o si el rostro no fue identificado claramente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a09b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de emociones:  ['asombro', 'felicidad', 'tristeza']\n",
      "Entrenando ( LBPH )...\n",
      "Tiempo de entrenamiento ( LBPH ):  16.691436767578125\n",
      "Modelo guardado en: C:\\Users\\Janin\\Documents\\IA\\emociones\\modeloLBPH.xml\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def obtenerModelo(method, facesData, labels, dataPath):\n",
    "    if method == 'LBPH':\n",
    "        emotion_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "        # Entrenando el reconocedor de rostros\n",
    "        print(\"Entrenando ( \"+method+\" )...\")\n",
    "        inicio = time.time()\n",
    "        emotion_recognizer.train(facesData, np.array(labels))\n",
    "        tiempoEntrenamiento = time.time()-inicio\n",
    "        print(\"Tiempo de entrenamiento ( \"+method+\" ): \", tiempoEntrenamiento)\n",
    "\n",
    "        # Almacenando el modelo obtenido\n",
    "        modelo_path = os.path.join(dataPath, \"modelo\"+method+\".xml\")\n",
    "        emotion_recognizer.write(modelo_path)\n",
    "        print(\"Modelo guardado en:\", modelo_path)\n",
    "\n",
    "dataPath = 'C:\\\\Users\\\\Janin\\\\Documents\\\\IA\\\\emociones'  # Cambia a la ruta donde hayas almacenado Data\n",
    "emotionsList = os.listdir(dataPath)\n",
    "print('Lista de emociones: ', emotionsList)\n",
    "\n",
    "labels = []\n",
    "facesData = []\n",
    "label = 0\n",
    "\n",
    "for nameDir in emotionsList:\n",
    "    emotionsPath = os.path.join(dataPath, nameDir)\n",
    "\n",
    "    for fileName in os.listdir(emotionsPath):\n",
    "        labels.append(label)\n",
    "        facesData.append(cv2.imread(os.path.join(emotionsPath, fileName), 0))\n",
    "    label += 1\n",
    "\n",
    "obtenerModelo('LBPH', facesData, labels, dataPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f95066",
   "metadata": {},
   "source": [
    "# Entrenamiento de reconocedor de emociones con LBPH\n",
    "\n",
    "En este script, implementé un sistema para entrenar un reconocedor de emociones utilizando la técnica LBPH (Local Binary Patterns Histograms) de OpenCV. Después de configurar el ambiente y cargar las imágenes de un directorio especificado, procedí a entrenar el modelo con estos datos. El modelo entrenado se guarda en un archivo XML dentro del mismo directorio de las imágenes. Este proceso incluye la captura de tiempos para evaluar la eficiencia del entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82aca643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagePaths= ['asombro', 'felicidad', 'modeloLBPH.xml', 'tristeza']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "method = 'LBPH'\n",
    "\n",
    "if method == 'LBPH': \n",
    "    emotion_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    emotion_recognizer.read('modelo'+method+'.xml')\n",
    "\n",
    "dataPath = 'C:\\\\Users\\\\Janin\\\\Documents\\\\IA\\\\emociones' # Cambia a la ruta donde hayas almacenado Data\n",
    "imagePaths = os.listdir(dataPath)\n",
    "print('imagePaths=',imagePaths)\n",
    "\n",
    "cap = cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "faceClassif = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False: \n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    auxFrame = gray.copy()\n",
    "\n",
    "    faces = faceClassif.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        rostro = auxFrame[y:y+h,x:x+w]\n",
    "        rostro = cv2.resize(rostro, (100, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        result = emotion_recognizer.predict(rostro)\n",
    "\n",
    "        cv2.putText(frame,'{}'.format(result),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
    "\n",
    "        # LBPH\n",
    "        if method == 'LBPH':\n",
    "            if result[1] < 60:\n",
    "                cv2.putText(frame,'{}'.format(imagePaths[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)\n",
    "                cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)\n",
    "            else:\n",
    "                cv2.putText(frame,'No identificado',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)\n",
    "                cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)\n",
    "\n",
    "    cv2.imshow('Frame',frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa12b4c",
   "metadata": {},
   "source": [
    "# Detección en tiempo real \n",
    "\n",
    "En este script, implemento un sistema de reconocimiento facial en tiempo real utilizando la biblioteca OpenCV. Primero, cargo un modelo preentrenado de reconocimiento de emociones usando LBPH. Luego, inicio la captura de video en tiempo real usando la webcam. Para cada cuadro capturado, detecto rostros y aplico el modelo para predecir la emoción. Los resultados se muestran en la pantalla con un cuadro alrededor del rostro y una etiqueta que indica la emoción detectada o si el rostro no fue identificado claramente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
